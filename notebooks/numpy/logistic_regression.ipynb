{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08b769cd-7f10-455b-936a-ec3d6b0c4679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import rand, randint, uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1b7a90-d547-4905-bd9e-0a1d83b469a0",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e47be3-0dc1-4d3a-afe9-8a4482e906f0",
   "metadata": {},
   "source": [
    "Logistic regession is a type of linear classification model. The idea is to compute the probability of a (usually) binary outcome by calculating a linear combination of different input values (= features). The linear combination is commonly called the logit and is passend to a sigmoid function. Note that while the decision boundary of logistic regression is linear in the feature space, the relationship between feature values and predicted probabilities is nonlinear.\n",
    "\n",
    "$P(y = 1 | x) = \\frac{1}{ 1 + e^{-(xw^\\top + b)}}$\n",
    "\n",
    "where $w$ and $b$ are model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be2606-a10c-4cda-b66a-fb98885cd8de",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0aa7c1d-2da9-4f96-b4e0-3511b9ad8b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rand(150, 4)\n",
    "y = randint(0, 2, 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bc87c4-5b7c-4ca9-a87d-061d792129d8",
   "metadata": {},
   "source": [
    "## Implement Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "55e0ce75-4ded-4128-9bbc-37fa3bd806b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def prob(x, w, b):\n",
    "    return sigmoid(np.dot(x, w) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55095552-0dd8-4d36-9e4f-bbd53a4f8691",
   "metadata": {},
   "source": [
    "As a loss function, we aim to use the cross entropy which is defined as\n",
    "\n",
    "$H(y, \\hat{y}) = y \\log(\\frac{1}{\\hat{y}}) + (1 - y) \\log(\\frac{1}{1 - \\hat{y}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "47855537-c34b-405f-8bd3-14ed0fcfe6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_true, y_pred):\n",
    "    return (y * np.log(1/y_pred) + (1 - y) * np.log(1 / (1 - y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "55e3c199-e88e-4a69-845a-ddb7b27a92f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "\n",
    "    def __init__(self, n_epochs: int):\n",
    "        self._n_epochs = n_epochs\n",
    "\n",
    "    def fit(self, X, y, lr=0.01):\n",
    "        n_samples = X.shape[0]\n",
    "        self.w = np.ones(X.shape[1])\n",
    "        self.b = 1\n",
    "\n",
    "        for i in range(self._n_epochs):\n",
    "            y_pred = prob(X, self.w, self.b)\n",
    "    \n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / n_samples) * np.sum(y_pred - y)\n",
    "    \n",
    "            self.w -= lr * dw\n",
    "            self.b -= lr * db\n",
    "\n",
    "        loss_after = cross_entropy(y, y_pred).mean()\n",
    "        print(loss_after)            \n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return prob(X, self.w, self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5e23903f-3f98-44ac-ba55-e7a50afd9953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.689849512203447\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(10000).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "42cccf80-05e5-48a7-bd0e-aa0541584276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.46174205, 0.52484484, 0.5015246 , 0.47113937, 0.45112005,\n",
       "       0.4791733 , 0.45705089, 0.49661943, 0.47141215, 0.44044461,\n",
       "       0.48584784, 0.51228096, 0.49923372, 0.46421701, 0.47198672,\n",
       "       0.45020637, 0.46886376, 0.50017205, 0.46367617, 0.51317321,\n",
       "       0.44804543, 0.45200138, 0.4587059 , 0.4602901 , 0.44794341,\n",
       "       0.46247757, 0.4898743 , 0.48428779, 0.45316083, 0.46417689,\n",
       "       0.52173621, 0.46813287, 0.48709019, 0.44860636, 0.4496718 ,\n",
       "       0.50139973, 0.47359144, 0.48143366, 0.48213527, 0.48286214,\n",
       "       0.50517267, 0.43954398, 0.46727843, 0.42892573, 0.46618513,\n",
       "       0.45061474, 0.50812395, 0.50117489, 0.50079967, 0.48154921,\n",
       "       0.49441695, 0.49724913, 0.46577479, 0.49027358, 0.49094945,\n",
       "       0.46214863, 0.51078777, 0.48081273, 0.45857779, 0.45345474,\n",
       "       0.47217172, 0.45803397, 0.51134581, 0.44615603, 0.45609825,\n",
       "       0.45655351, 0.44792969, 0.46131672, 0.48099104, 0.43700109,\n",
       "       0.4719436 , 0.45975328, 0.4662103 , 0.49611247, 0.48739   ,\n",
       "       0.47137443, 0.4784393 , 0.48666747, 0.49703781, 0.46009452,\n",
       "       0.46029255, 0.47401592, 0.50218583, 0.51843863, 0.43665957,\n",
       "       0.5079892 , 0.44700978, 0.49314158, 0.45908718, 0.4422608 ,\n",
       "       0.44829334, 0.51194874, 0.46948734, 0.46376736, 0.41932185,\n",
       "       0.52755348, 0.48291365, 0.48441427, 0.46752182, 0.50710827,\n",
       "       0.5054532 , 0.44493731, 0.48375073, 0.47781597, 0.48691308,\n",
       "       0.49513667, 0.48017918, 0.48273006, 0.4976969 , 0.50420452,\n",
       "       0.45584346, 0.48938302, 0.48075743, 0.47364929, 0.45682227,\n",
       "       0.48213465, 0.43445833, 0.44321122, 0.46206203, 0.49984609,\n",
       "       0.44472401, 0.51706189, 0.48138942, 0.44905491, 0.53050406,\n",
       "       0.50269336, 0.50787585, 0.44724379, 0.45087952, 0.45810223,\n",
       "       0.4912881 , 0.49934218, 0.47349607, 0.45771368, 0.49679175,\n",
       "       0.47711346, 0.48243293, 0.48991312, 0.48126323, 0.47986336,\n",
       "       0.49506418, 0.4901241 , 0.46045322, 0.48490424, 0.46122753,\n",
       "       0.52033722, 0.49523462, 0.45346471, 0.4489873 , 0.50317097])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cb6362-1714-492e-808b-8363f5d19aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
